{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c280c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "print(f'Cuda is on: {cuda}')\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a7299d",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed59ab95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CausalConv2d(torch.nn.Conv2d):\n",
    "    \n",
    "    ''' Causal Convolutional Neural Network in 1-D. Basically a special case of CNN-1d\n",
    "    so its generator inherents from torch.nn.Conv1d class.\n",
    "\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size,\n",
    "                 stride = 1,\n",
    "                 dilation = 1,\n",
    "                 groups = 1,\n",
    "                 bias = True):\n",
    "        \n",
    "        super(CausalConv2d, self).__init__(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size = kernel_size,\n",
    "            padding = 0,\n",
    "            stride = stride,\n",
    "            dilation = dilation,\n",
    "            groups = groups,\n",
    "            bias = True)\n",
    "        \n",
    "        self.__padding = (kernel_size[1] - 1) * dilation\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return super(CausalConv2d, self).forward(F.pad(input, (0,self.__padding)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a0777c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5656edd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class context_embedding(torch.nn.Module):\n",
    "    \n",
    "    '''\n",
    "       Embedding the context. You can understand this as construct a model using causal CNN.\n",
    "       However, the return value is sigmoided.\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 in_channels = 1,\n",
    "                 embedding_size = 256,\n",
    "                 k = 5):\n",
    "        super(context_embedding, self).__init__()\n",
    "        \n",
    "        self.causal_convolution = CausalConv2d(in_channels,\n",
    "                                               embedding_size,\n",
    "                                               kernel_size = k)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.causal_convolution(x)\n",
    "        \n",
    "        return F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d94489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetData(Dataset):\n",
    "    \n",
    "    '''\n",
    "        Get dataset \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,target,feature,t0=96,to_predict = 24,transform=None):\n",
    "        \n",
    "        self.t0 = t0 # known time step\n",
    "        self.transform = None\n",
    "        \n",
    "        length = target.shape[0]\n",
    "        step = t0+to_predict ##step in one data point (known + topredict)\n",
    "        \n",
    "        eff_len = length - step + 1 ## effective length of the whole time series i.e. can be break in to how many data points\n",
    "        \n",
    "        Y = []\n",
    "        F = []\n",
    "        \n",
    "        for i in range(eff_len):\n",
    "            Y.append(target[i:step+i].unsqueeze(0))## size * time step\n",
    "            F.append(feature[i:step+i].unsqueeze(0).permute(0,2,1)) ## size * feature size * time step\n",
    "        \n",
    "        self.target = torch.cat(Y) ## size * dimension\n",
    "        self.feature = torch.cat(F)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.time = torch.cat(eff_len*[torch.arange(0,step).type(torch.float).unsqueeze(0)])\n",
    "        \n",
    "        \n",
    "        self.masks = self._generate_square_subsequent_mask(t0)\n",
    "                \n",
    "        \n",
    "        # print out shapes to confirm desired output\n",
    "        print(self.time.shape)   \n",
    "        print(self.target.shape)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.target.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        \n",
    "        sample = (self.time[idx,:],\n",
    "                  self.target[idx,:],\n",
    "                  self.feature[idx,:],\n",
    "                  self.masks) #in tuple shape, will be called in training, eval and testing.\n",
    "        \n",
    "        if self.transform:\n",
    "            sample=self.transform(sample)\n",
    "            \n",
    "        return sample\n",
    "    \n",
    "    def _generate_square_subsequent_mask(self,t0):\n",
    "        mask = torch.zeros(t0+to_predict,t0+to_predict)\n",
    "        for i in range(0,t0):\n",
    "            mask[i,t0:] = 1 \n",
    "        for i in range(t0,t0+to_predict):\n",
    "            mask[i,i:] = 1\n",
    "        mask = mask.float().masked_fill(mask == 1, float('-inf'))#.masked_fill(mask == 1, float(0.0))\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bb87c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model,train_dl,optimizer,t0=96,to_predict = 24,):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    n = 0\n",
    "    for step,(x,y,feature,attention_masks) in enumerate(train_dl): # using dataloader, a library in pytorch. It helps \n",
    "                                                           # carry out batching traing\n",
    "                                                            # google torch.utils.data.DataLoader\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        feature = feature.to(device)\n",
    "        attention_masks = attention_masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x,y,feature,attention_masks[0])\n",
    "\n",
    "        loss = criterion(output.squeeze()[:,t0:],y[:,t0:]) # not missing data\n",
    "    \n",
    "        #loss = criterion(output.squeeze(),y)\n",
    "        # loss = criterion(output.squeeze()[:,(t0-1-10):(t0+24-1-10)],y.cuda()[:,(t0-10):]) # missing data\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += (loss.detach().item() * x.shape[0])\n",
    "        n += x.shape[0]\n",
    "    return train_loss/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ee984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(model,test_dl,t0=96,to_predict = 24):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    n = 0\n",
    "    with torch.no_grad(): # eliminate gradient i.e. gradients only exists in train()\n",
    "        \n",
    "        for step,(x,y,feature,attention_masks) in enumerate(test_dl):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            feature = feature.to(device)\n",
    "            attention_masks = attention_masks.to(device)\n",
    "            output = model(x,y,feature,attention_masks[0])\n",
    "            \n",
    "            #loss = criterion(output.squeeze()[:,t0:],y[:,t0:]) # not missing data\n",
    "            loss = torch.nn.L1Loss()(output.squeeze()[:,t0:],y[:,t0:])\n",
    "            # loss = criterion(output.squeeze()[:,(t0-1-10):(t0+24-1-10)],y.cuda()[:,(t0-10):]) # missing data\n",
    "            \n",
    "            test_loss += (loss.detach().item() * x.shape[0])\n",
    "            n += x.shape[0]\n",
    "            \n",
    "    return test_loss/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd021865",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "\n",
    "class TransformerTimeSeries(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Time Series application of transformers based on paper\n",
    "    \n",
    "    causal_convolution_layer parameters:\n",
    "        in_channels: the number of features per time point\n",
    "        out_channels: the number of features outputted per time point\n",
    "        kernel_size: k is the width of the 1-D sliding kernel\n",
    "        \n",
    "    nn.Transformer parameters:\n",
    "        d_model: the size of the embedding vector (input)\n",
    "    \n",
    "    PositionalEncoding parameters:\n",
    "        d_model: the size of the embedding vector (positional vector)\n",
    "        dropout: the dropout to be used on the sum of positional+embedding vector\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,feature_dim,nhead = 16,num_layers = 8,d_model = 512,conv_len = 5,feature_weight = True,embedding_weight = True):\n",
    "        super(TransformerTimeSeries,self).__init__()\n",
    "        self.feature_weight = feature_weight\n",
    "        self.embedding_weight = embedding_weight\n",
    "        self.feature_dim = feature_dim\n",
    "        self.input_embedding_y = context_embedding(1,d_model,(1,conv_len))\n",
    "        \n",
    "        if feature_weight:\n",
    "            self.input_embedding_f = context_embedding(1,d_model,(1,conv_len))\n",
    "            self.feature_projection = torch.nn.Linear(feature_dim,1)\n",
    "        else:\n",
    "            self.input_embedding_f = context_embedding(1,d_model,(feature_dim,conv_len))\n",
    "        \n",
    "        \n",
    "        \n",
    "        if embedding_weight:\n",
    "            self.embbeding_projection = torch.nn.Linear(3,1)\n",
    "            \n",
    "        self.bn = torch.nn.BatchNorm2d(1)\n",
    "        self.positional_embedding = torch.nn.Embedding(2*d_model,d_model)\n",
    "        self.decode_layer = torch.nn.TransformerEncoderLayer(d_model=d_model,nhead=nhead)\n",
    "        self.transformer_decoder = torch.nn.TransformerEncoder(self.decode_layer, num_layers = num_layers)\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(d_model,1)\n",
    "\n",
    "        \n",
    "    def forward(self,x,y,feature,attention_masks):\n",
    "        \n",
    "        \n",
    "        z = y.unsqueeze(1).unsqueeze(1) ## reshape it so it can be processed by 2D CNN\n",
    "        z = self.bn(z) ##Batch Norm\n",
    "        \n",
    "        if self.feature_dim > 1:    \n",
    "            f = feature.unsqueeze(1)## embedding of nulti-dim feature\n",
    "        \n",
    "        else:\n",
    "            f = feature.unsqueeze(1).unsqueeze(1) ## One dim feature \n",
    "        \n",
    "        # input_embedding returns shape (Batch size,embedding size,sequence len) -> need (sequence len,Batch size,embedding_size)\n",
    "        z_embedding = self.input_embedding_y(z).squeeze().permute(2,0,1) ## embedding of target\n",
    "        \n",
    "        if self.feature_weight:\n",
    "            \n",
    "            f_embedding = self.input_embedding_f(f).squeeze()\n",
    "            f_embedding = f_embedding.permute(0,1,3,2) ## multi dim feature\n",
    "            f_embedding = self.feature_projection(f_embedding).squeeze() ## linear combination of features' embeddings\n",
    "            f_embedding = f_embedding.permute(2,0,1)\n",
    "            \n",
    "        else:\n",
    "            f_embedding = self.input_embedding_f(f).squeeze().permute(2,0,1)\n",
    "            \n",
    "        # get my positional embeddings (Batch size, sequence_len, embedding_size) -> need (sequence len,Batch size,embedding_size)\n",
    "        positional_embeddings = self.positional_embedding(x.type(torch.long)).permute(1,0,2)\n",
    "        \n",
    "    \n",
    "        if self.embedding_weight:\n",
    "            all_embedding = torch.cat([z_embedding.unsqueeze(3),positional_embeddings.unsqueeze(3),f_embedding.unsqueeze(3)],3)\n",
    "            input_embedding = self.embbeding_projection(all_embedding)## the linear combination of three embeddings\n",
    "            input_embedding = input_embedding.squeeze()\n",
    "        else:\n",
    "            input_embedding = z_embedding + positional_embeddings + f_embedding ## add up directly\n",
    "        \n",
    "        transformer_embedding = self.transformer_decoder(input_embedding,attention_masks)\n",
    "        \n",
    "        output = self.fc1(transformer_embedding.permute(1,0,2))\n",
    "        \n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3accefd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "923c2805",
   "metadata": {},
   "source": [
    "# 5 day predict one day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c761ba",
   "metadata": {},
   "source": [
    "## AAPL\n",
    "##### open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ccac02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_name = 'AAPL_features.csv'\n",
    "with open(file_name) as f:\n",
    "    data_list = csv.reader(f)\n",
    "    data = []\n",
    "    for row in data_list:\n",
    "        data.append(row)\n",
    "    f.close()\n",
    "    \n",
    "data = torch.tensor(np.delete(np.array(data)[1:,2:],4,axis = 1).astype(np.float)).type(torch.float)\n",
    "\n",
    "feature_dim = data.shape[1] - 1\n",
    "rate = []\n",
    "for i in range(1,data.shape[0]):\n",
    "    rate.append((100*(data[i,0:4] - data[i-1,0:4])/data[i-1,0:4]).reshape(1,4))\n",
    "    \n",
    "rate = torch.cat(rate)\n",
    "rate_data = torch.cat([rate,data[1:,4:]],1)\n",
    "train_size = int(data.shape[0]*0.9)\n",
    "data_train = rate_data[0:train_size]\n",
    "data_test = rate_data[train_size-4:]\n",
    "\n",
    "\n",
    "t0 = 5\n",
    "to_predict = 1\n",
    "train_dataset = GetData(data_train[:,0],data_train[:,1:],t0 = t0, to_predict = to_predict)\n",
    "test_dataset = GetData(data_test[:,0],data_test[:,1:],t0 = t0, to_predict = to_predict)\n",
    "criterion = torch.nn.MSELoss()\n",
    "# dataloaders\n",
    "train_dl = DataLoader(train_dataset,batch_size=256,shuffle=True)\n",
    "test_dl = DataLoader(test_dataset,batch_size=512)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = TransformerTimeSeries(feature_dim = feature_dim,\n",
    "                              nhead = 4,num_layers = 12,d_model = 512,\n",
    "                              conv_len = 5,feature_weight = True,embedding_weight = True)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5 ,momentum = 0.9, nesterov = True)#Adam optimizer\n",
    "#scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer,5e-7,3e-5,10,mode = 'exp_range',gamma = 0.9996)\n",
    "\n",
    "\n",
    "\n",
    "train_epoch_loss = []\n",
    "test_epoch_loss = []\n",
    "test_best = 9999\n",
    "for epoch in tqdm(range(4000)):\n",
    "\n",
    "\n",
    "    l_t = train_epoch(model,train_dl,optimizer,t0,to_predict)\n",
    "\n",
    "    train_loss = test_epoch(model,train_dl,t0,to_predict)\n",
    "\n",
    "\n",
    "    test_loss = test_epoch(model,test_dl,t0,to_predict)\n",
    "\n",
    "\n",
    "    if test_best > test_loss:\n",
    "        test_best = test_loss\n",
    "        torch.save(model.state_dict(), 'AAPL')\n",
    "\n",
    "\n",
    "\n",
    "    train_epoch_loss.append(np.mean(train_loss))\n",
    "    test_epoch_loss.append(test_loss)\n",
    "\n",
    "    print(\"Epoch {}: Train loss: {} \\t test loss: {}\".format(epoch+1,\n",
    "                                                             np.mean(train_loss),\n",
    "                                                             test_loss))\n",
    "    #scheduler.step()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e119672a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bmodel = TransformerTimeSeries(feature_dim = feature_dim,\n",
    "                                  nhead = 4,num_layers = 12,d_model = 512,\n",
    "                                  conv_len = 5,feature_weight = True,embedding_weight = True)\n",
    "bmodel.load_state_dict(torch.load('AAPL'))\n",
    "bmodel.to(device)\n",
    "bmodel.eval()\n",
    "with torch.no_grad(): # eliminate gradient i.e. gradients only exists in train()\n",
    "\n",
    "    for step,(x,y,feature,attention_masks) in enumerate(test_dl):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        feature = feature.to(device)\n",
    "        attention_masks = attention_masks.to(device)\n",
    "        output = bmodel(x,y,feature,attention_masks[0])\n",
    "        \n",
    "        pred = data[train_size + 1 :-1,0]*0.01*(100+output[:,t0:].squeeze()).detach().cpu()\n",
    "        ## note that test set starts at index train_size + 1 because idex train_size in rate_data \n",
    "        ## is the last day of first test data point, but rate_data only have the second til last data points\n",
    "        ## of the dataset.\n",
    "        real = data[train_size + 1 :-1,0]*0.01*(100+y[:,t0:].squeeze()).detach().cpu()\n",
    "\n",
    "        plt.plot(pred)\n",
    "        plt.plot(real)\n",
    "        plt.legend(['pred','real'])\n",
    "        MAE = torch.nn.L1Loss()(pred,real).item()\n",
    "        MSE = torch.nn.MSELoss()(pred,real).item()\n",
    "        RMSE = np.sqrt(MSE)\n",
    "        \n",
    "        print(f'AAPL MAE is {MAE}')\n",
    "        print(f'AAPL MSE is {MSE}')\n",
    "        print(f'AAPL RMSE is {RMSE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeefbe5",
   "metadata": {},
   "source": [
    "##### High"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35010484",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'AAPL_features.csv'\n",
    "with open(file_name) as f:\n",
    "    data_list = csv.reader(f)\n",
    "    data = []\n",
    "    for row in data_list:\n",
    "        data.append(row)\n",
    "    f.close()\n",
    "    \n",
    "data = torch.tensor(np.delete(np.array(data)[1:,2:],4,axis = 1).astype(np.float)).type(torch.float)\n",
    "\n",
    "feature_dim = data.shape[1] - 1\n",
    "rate = []\n",
    "for i in range(1,data.shape[0]):\n",
    "    rate.append((100*(data[i,0:4] - data[i-1,0:4])/data[i-1,0:4]).reshape(1,4))\n",
    "    \n",
    "rate = torch.cat(rate)\n",
    "rate_data = torch.cat([rate,data[1:,4:]],1)\n",
    "train_size = int(data.shape[0]*0.9)\n",
    "data_train = rate_data[0:train_size]\n",
    "data_test = rate_data[train_size-4:]\n",
    "\n",
    "\n",
    "t0 = 5\n",
    "to_predict = 1\n",
    "train_dataset = GetData(data_train[:,1],torch.cat([data_train[:,0].unsqueeze(1),data_train[:,2:]],1),t0 = t0, to_predict = to_predict)\n",
    "test_dataset = GetData(data_test[:,1],torch.cat([data_test[:,0].unsqueeze(1),data_test[:,2:]],1),t0 = t0, to_predict = to_predict)\n",
    "criterion = torch.nn.MSELoss()\n",
    "# dataloaders\n",
    "train_dl = DataLoader(train_dataset,batch_size=256,shuffle=True)\n",
    "test_dl = DataLoader(test_dataset,batch_size=512)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = TransformerTimeSeries(feature_dim = feature_dim,\n",
    "                              nhead = 4,num_layers = 12,d_model = 512,\n",
    "                              conv_len = 5,feature_weight = True,embedding_weight = True)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5 ,momentum = 0.9, nesterov = True)#Adam optimizer\n",
    "#scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer,5e-7,3e-5,10,mode = 'exp_range',gamma = 0.9996)\n",
    "\n",
    "\n",
    "\n",
    "train_epoch_loss = []\n",
    "test_epoch_loss = []\n",
    "test_best = 9999\n",
    "for epoch in tqdm(range(4000)):\n",
    "\n",
    "\n",
    "    l_t = train_epoch(model,train_dl,optimizer,t0,to_predict)\n",
    "\n",
    "    train_loss = test_epoch(model,train_dl,t0,to_predict)\n",
    "\n",
    "\n",
    "    test_loss = test_epoch(model,test_dl,t0,to_predict)\n",
    "\n",
    "\n",
    "    if test_best > test_loss:\n",
    "        test_best = test_loss\n",
    "        torch.save(model.state_dict(), 'AAPL_H')\n",
    "\n",
    "\n",
    "\n",
    "    train_epoch_loss.append(np.mean(train_loss))\n",
    "    test_epoch_loss.append(test_loss)\n",
    "\n",
    "    print(\"Epoch {}: Train loss: {} \\t test loss: {}\".format(epoch+1,\n",
    "                                                             np.mean(train_loss),\n",
    "                                                             test_loss))\n",
    "    #scheduler.step()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4117f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bmodel = TransformerTimeSeries(feature_dim = feature_dim,\n",
    "                                  nhead = 4,num_layers = 12,d_model = 512,\n",
    "                                  conv_len = 5,feature_weight = True,embedding_weight = True)\n",
    "bmodel.load_state_dict(torch.load('AAPL_H'))\n",
    "bmodel.to(device)\n",
    "bmodel.eval()\n",
    "with torch.no_grad(): # eliminate gradient i.e. gradients only exists in train()\n",
    "\n",
    "    for step,(x,y,feature,attention_masks) in enumerate(test_dl):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        feature = feature.to(device)\n",
    "        attention_masks = attention_masks.to(device)\n",
    "        output = bmodel(x,y,feature,attention_masks[0])\n",
    "        \n",
    "        pred = data[train_size + 1 :-1,1]*0.01*(100+output[:,t0:].squeeze()).detach().cpu()\n",
    "        ## note that test set starts at index train_size + 1 because idex train_size in rate_data \n",
    "        ## is the last day of first test data point, but rate_data only have the second til last data points\n",
    "        ## of the dataset.\n",
    "        real = data[train_size + 1 :-1,1]*0.01*(100+y[:,t0:].squeeze()).detach().cpu()\n",
    "\n",
    "        plt.plot(pred)\n",
    "        plt.plot(real)\n",
    "        plt.legend(['pred','real'])\n",
    "        MAE = torch.nn.L1Loss()(pred,real).item()\n",
    "        MSE = torch.nn.MSELoss()(pred,real).item()\n",
    "        RMSE = np.sqrt(MSE)\n",
    "        \n",
    "        print(f'AAPL MAE is {MAE}')\n",
    "        print(f'AAPL MSE is {MSE}')\n",
    "        print(f'AAPL RMSE is {RMSE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3be475",
   "metadata": {},
   "source": [
    "##### low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dce1d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_name = 'AAPL_features.csv'\n",
    "with open(file_name) as f:\n",
    "    data_list = csv.reader(f)\n",
    "    data = []\n",
    "    for row in data_list:\n",
    "        data.append(row)\n",
    "    f.close()\n",
    "    \n",
    "data = torch.tensor(np.delete(np.array(data)[1:,2:],4,axis = 1).astype(np.float)).type(torch.float)\n",
    "\n",
    "feature_dim = data.shape[1] - 1\n",
    "rate = []\n",
    "for i in range(1,data.shape[0]):\n",
    "    rate.append((100*(data[i,0:4] - data[i-1,0:4])/data[i-1,0:4]).reshape(1,4))\n",
    "    \n",
    "rate = torch.cat(rate)\n",
    "rate_data = torch.cat([rate,data[1:,4:]],1)\n",
    "train_size = int(data.shape[0]*0.9)\n",
    "data_train = rate_data[0:train_size]\n",
    "data_test = rate_data[train_size-4:]\n",
    "\n",
    "\n",
    "t0 = 5\n",
    "to_predict = 1\n",
    "train_dataset = GetData(data_train[:,2],torch.cat([data_train[:,0:2],data_train[:,3:]],1),t0 = t0, to_predict = to_predict)\n",
    "test_dataset = GetData(data_test[:,2],torch.cat([data_test[:,0:2],data_test[:,3:]],1),t0 = t0, to_predict = to_predict)\n",
    "criterion = torch.nn.MSELoss()\n",
    "# dataloaders\n",
    "train_dl = DataLoader(train_dataset,batch_size=256,shuffle=True)\n",
    "test_dl = DataLoader(test_dataset,batch_size=512)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = TransformerTimeSeries(feature_dim = feature_dim,\n",
    "                              nhead = 4,num_layers = 12,d_model = 512,\n",
    "                              conv_len = 5,feature_weight = True,embedding_weight = True)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5 ,momentum = 0.9, nesterov = True)#Adam optimizer\n",
    "#scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer,5e-7,3e-5,10,mode = 'exp_range',gamma = 0.9996)\n",
    "\n",
    "\n",
    "\n",
    "train_epoch_loss = []\n",
    "test_epoch_loss = []\n",
    "test_best = 9999\n",
    "for epoch in tqdm(range(4000)):\n",
    "\n",
    "\n",
    "    l_t = train_epoch(model,train_dl,optimizer,t0,to_predict)\n",
    "\n",
    "    train_loss = test_epoch(model,train_dl,t0,to_predict)\n",
    "\n",
    "\n",
    "    test_loss = test_epoch(model,test_dl,t0,to_predict)\n",
    "\n",
    "\n",
    "    if test_best > test_loss:\n",
    "        test_best = test_loss\n",
    "        torch.save(model.state_dict(), 'AAPL_L')\n",
    "\n",
    "\n",
    "\n",
    "    train_epoch_loss.append(np.mean(train_loss))\n",
    "    test_epoch_loss.append(test_loss)\n",
    "\n",
    "    print(\"Epoch {}: Train loss: {} \\t test loss: {}\".format(epoch+1,\n",
    "                                                             np.mean(train_loss),\n",
    "                                                             test_loss))\n",
    "    #scheduler.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f89c90c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dced9c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "bmodel = TransformerTimeSeries(feature_dim = feature_dim,\n",
    "                                  nhead = 4,num_layers = 12,d_model = 512,\n",
    "                                  conv_len = 5,feature_weight = True,embedding_weight = True)\n",
    "bmodel.load_state_dict(torch.load('AAPL_L'))\n",
    "bmodel.to(device)\n",
    "bmodel.eval()\n",
    "with torch.no_grad(): # eliminate gradient i.e. gradients only exists in train()\n",
    "\n",
    "    for step,(x,y,feature,attention_masks) in enumerate(test_dl):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        feature = feature.to(device)\n",
    "        attention_masks = attention_masks.to(device)\n",
    "        output = bmodel(x,y,feature,attention_masks[0])\n",
    "        \n",
    "        pred = data[train_size + 1 :-1,2]*0.01*(100+output[:,t0:].squeeze()).detach().cpu()\n",
    "        ## note that test set starts at index train_size + 1 because idex train_size in rate_data \n",
    "        ## is the last day of first test data point, but rate_data only have the second til last data points\n",
    "        ## of the dataset.\n",
    "        real = data[train_size + 1 :-1,2]*0.01*(100+y[:,t0:].squeeze()).detach().cpu()\n",
    "\n",
    "        plt.plot(pred)\n",
    "        plt.plot(real)\n",
    "        plt.legend(['pred','real'])\n",
    "        MAE = torch.nn.L1Loss()(pred,real).item()\n",
    "        MSE = torch.nn.MSELoss()(pred,real).item()\n",
    "        RMSE = np.sqrt(MSE)\n",
    "        \n",
    "        print(f'AAPL MAE is {MAE}')\n",
    "        print(f'AAPL MSE is {MSE}')\n",
    "        print(f'AAPL RMSE is {RMSE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1d013d",
   "metadata": {},
   "source": [
    "##### close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6261d45f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a89cd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'AAPL_features.csv'\n",
    "with open(file_name) as f:\n",
    "    data_list = csv.reader(f)\n",
    "    data = []\n",
    "    for row in data_list:\n",
    "        data.append(row)\n",
    "    f.close()\n",
    "    \n",
    "data = torch.tensor(np.delete(np.array(data)[1:,2:],4,axis = 1).astype(np.float)).type(torch.float)\n",
    "\n",
    "feature_dim = data.shape[1] - 1\n",
    "rate = []\n",
    "for i in range(1,data.shape[0]):\n",
    "    rate.append((100*(data[i,0:4] - data[i-1,0:4])/data[i-1,0:4]).reshape(1,4))\n",
    "    \n",
    "rate = torch.cat(rate)\n",
    "rate_data = torch.cat([rate,data[1:,4:]],1)\n",
    "train_size = int(data.shape[0]*0.9)\n",
    "data_train = rate_data[0:train_size]\n",
    "data_test = rate_data[train_size-4:]\n",
    "\n",
    "\n",
    "t0 = 5\n",
    "to_predict = 1\n",
    "train_dataset = GetData(data_train[:,3],torch.cat([data_train[:,0:3],data_train[:,4:]],1),t0 = t0, to_predict = to_predict)\n",
    "test_dataset = GetData(data_test[:,3],torch.cat([data_test[:,0:3],data_test[:,4:]],1),t0 = t0, to_predict = to_predict)\n",
    "criterion = torch.nn.MSELoss()\n",
    "# dataloaders\n",
    "train_dl = DataLoader(train_dataset,batch_size=256,shuffle=True)\n",
    "test_dl = DataLoader(test_dataset,batch_size=512)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = TransformerTimeSeries(feature_dim = feature_dim,\n",
    "                              nhead = 4,num_layers = 12,d_model = 512,\n",
    "                              conv_len = 5,feature_weight = True,embedding_weight = True)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5 ,momentum = 0.9, nesterov = True)#Adam optimizer\n",
    "#scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer,5e-7,3e-5,10,mode = 'exp_range',gamma = 0.9996)\n",
    "\n",
    "\n",
    "\n",
    "train_epoch_loss = []\n",
    "test_epoch_loss = []\n",
    "test_best = 9999\n",
    "for epoch in tqdm(range(4000)):\n",
    "\n",
    "\n",
    "    l_t = train_epoch(model,train_dl,optimizer,t0,to_predict)\n",
    "\n",
    "    train_loss = test_epoch(model,train_dl,t0,to_predict)\n",
    "\n",
    "\n",
    "    test_loss = test_epoch(model,test_dl,t0,to_predict)\n",
    "\n",
    "\n",
    "    if test_best > test_loss:\n",
    "        test_best = test_loss\n",
    "        torch.save(model.state_dict(), 'AAPL_C')\n",
    "\n",
    "\n",
    "\n",
    "    train_epoch_loss.append(np.mean(train_loss))\n",
    "    test_epoch_loss.append(test_loss)\n",
    "\n",
    "    print(\"Epoch {}: Train loss: {} \\t test loss: {}\".format(epoch+1,\n",
    "                                                             np.mean(train_loss),\n",
    "                                                             test_loss))\n",
    "    #scheduler.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d2fe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "bmodel = TransformerTimeSeries(feature_dim = feature_dim,\n",
    "                                  nhead = 4,num_layers = 12,d_model = 512,\n",
    "                                  conv_len = 5,feature_weight = True,embedding_weight = True)\n",
    "bmodel.load_state_dict(torch.load('AAPL_C'))\n",
    "bmodel.to(device)\n",
    "bmodel.eval()\n",
    "with torch.no_grad(): # eliminate gradient i.e. gradients only exists in train()\n",
    "\n",
    "    for step,(x,y,feature,attention_masks) in enumerate(test_dl):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        feature = feature.to(device)\n",
    "        attention_masks = attention_masks.to(device)\n",
    "        output = bmodel(x,y,feature,attention_masks[0])\n",
    "        \n",
    "        pred = data[train_size + 1 :-1,3]*0.01*(100+output[:,t0:].squeeze()).detach().cpu()\n",
    "        ## note that test set starts at index train_size + 1 because idex train_size in rate_data \n",
    "        ## is the last day of first test data point, but rate_data only have the second til last data points\n",
    "        ## of the dataset.\n",
    "        real = data[train_size + 1 :-1,3]*0.01*(100+y[:,t0:].squeeze()).detach().cpu()\n",
    "\n",
    "        plt.plot(pred)\n",
    "        plt.plot(real)\n",
    "        plt.legend(['pred','real'])\n",
    "        MAE = torch.nn.L1Loss()(pred,real).item()\n",
    "        MSE = torch.nn.MSELoss()(pred,real).item()\n",
    "        RMSE = np.sqrt(MSE)\n",
    "        \n",
    "        print(f'AAPL MAE is {MAE}')\n",
    "        print(f'AAPL MSE is {MSE}')\n",
    "        print(f'AAPL RMSE is {RMSE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7749e30c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62afdde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75639900",
   "metadata": {},
   "source": [
    "## AMZN\n",
    "##### open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69f171a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_name = 'AMZN_features.csv'\n",
    "with open(file_name) as f:\n",
    "    data_list = csv.reader(f)\n",
    "    data = []\n",
    "    for row in data_list:\n",
    "        data.append(row)\n",
    "    f.close()\n",
    "    \n",
    "data = torch.tensor(np.delete(np.array(data)[1:,2:],4,axis = 1).astype(np.float)).type(torch.float)\n",
    "\n",
    "feature_dim = data.shape[1] - 1\n",
    "rate = []\n",
    "for i in range(1,data.shape[0]):\n",
    "    rate.append((100*(data[i,0:4] - data[i-1,0:4])/data[i-1,0:4]).reshape(1,4))\n",
    "    \n",
    "rate = torch.cat(rate)\n",
    "rate_data = torch.cat([rate,data[1:,4:]],1)\n",
    "train_size = int(data.shape[0]*0.9)\n",
    "data_train = rate_data[0:train_size]\n",
    "data_test = rate_data[train_size-4:]\n",
    "\n",
    "\n",
    "t0 = 5\n",
    "to_predict = 1\n",
    "train_dataset = GetData(data_train[:,0],data_train[:,1:],t0 = t0, to_predict = to_predict)\n",
    "test_dataset = GetData(data_test[:,0],data_test[:,1:],t0 = t0, to_predict = to_predict)\n",
    "criterion = torch.nn.MSELoss()\n",
    "# dataloaders\n",
    "train_dl = DataLoader(train_dataset,batch_size=256,shuffle=True)\n",
    "test_dl = DataLoader(test_dataset,batch_size=512)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = TransformerTimeSeries(feature_dim = feature_dim,\n",
    "                              nhead = 4,num_layers = 12,d_model = 512,\n",
    "                              conv_len = 5,feature_weight = True,embedding_weight = True)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5 ,momentum = 0.9, nesterov = True)#Adam optimizer\n",
    "#scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer,5e-7,3e-5,10,mode = 'exp_range',gamma = 0.9996)\n",
    "\n",
    "\n",
    "\n",
    "train_epoch_loss = []\n",
    "test_epoch_loss = []\n",
    "test_best = 9999\n",
    "for epoch in tqdm(range(4000)):\n",
    "\n",
    "\n",
    "    l_t = train_epoch(model,train_dl,optimizer,t0,to_predict)\n",
    "\n",
    "    train_loss = test_epoch(model,train_dl,t0,to_predict)\n",
    "\n",
    "\n",
    "    test_loss = test_epoch(model,test_dl,t0,to_predict)\n",
    "\n",
    "\n",
    "    if test_best > test_loss:\n",
    "        test_best = test_loss\n",
    "        torch.save(model.state_dict(), 'AMZN')\n",
    "\n",
    "\n",
    "\n",
    "    train_epoch_loss.append(np.mean(train_loss))\n",
    "    test_epoch_loss.append(test_loss)\n",
    "\n",
    "    print(\"Epoch {}: Train loss: {} \\t test loss: {}\".format(epoch+1,\n",
    "                                                             np.mean(train_loss),\n",
    "                                                             test_loss))\n",
    "    #scheduler.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca28779",
   "metadata": {},
   "outputs": [],
   "source": [
    "bmodel = TransformerTimeSeries(feature_dim = feature_dim,\n",
    "                                  nhead = 4,num_layers = 12,d_model = 512,\n",
    "                                  conv_len = 5,feature_weight = True,embedding_weight = True)\n",
    "bmodel.load_state_dict(torch.load('AMZN'))\n",
    "bmodel.to(device)\n",
    "bmodel.eval()\n",
    "with torch.no_grad(): # eliminate gradient i.e. gradients only exists in train()\n",
    "\n",
    "    for step,(x,y,feature,attention_masks) in enumerate(test_dl):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        feature = feature.to(device)\n",
    "        \n",
    "        #y = torch.cat([y[:,0:5],torch.ones(47,1).cuda()],1)\n",
    "        #feature = torch.cat([feature[:,:,0:5],torch.ones(47,90,1).cuda()],2)\n",
    "        \n",
    "        attention_masks = attention_masks.to(device)\n",
    "        output = bmodel(x,y,feature,attention_masks[0])\n",
    "        \n",
    "        pred = data[train_size + 1 :-1,0]*0.01*(100+output[:,t0:].squeeze()).detach().cpu()\n",
    "        real = data[train_size + 1 :-1,0]*0.01*(100+y[:,t0:].squeeze()).detach().cpu()\n",
    "\n",
    "        plt.plot(pred)\n",
    "        plt.plot(real)\n",
    "        plt.legend(['pred','real'])\n",
    "        MAE = torch.nn.L1Loss()(pred,real).item()\n",
    "        MSE = torch.nn.MSELoss()(pred,real).item()\n",
    "        RMSE = np.sqrt(MSE)\n",
    "        \n",
    "        print(f'AMZN MAE is {MAE}')\n",
    "        print(f'AMZN MSE is {MSE}')\n",
    "        print(f'AMZN RMSE is {RMSE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1ceea0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb8eb1f9",
   "metadata": {},
   "source": [
    "##### High"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccc3c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'AMZN_features.csv'\n",
    "with open(file_name) as f:\n",
    "    data_list = csv.reader(f)\n",
    "    data = []\n",
    "    for row in data_list:\n",
    "        data.append(row)\n",
    "    f.close()\n",
    "    \n",
    "data = torch.tensor(np.delete(np.array(data)[1:,2:],4,axis = 1).astype(np.float)).type(torch.float)\n",
    "\n",
    "feature_dim = data.shape[1] - 1\n",
    "rate = []\n",
    "for i in range(1,data.shape[0]):\n",
    "    rate.append((100*(data[i,0:4] - data[i-1,0:4])/data[i-1,0:4]).reshape(1,4))\n",
    "    \n",
    "rate = torch.cat(rate)\n",
    "rate_data = torch.cat([rate,data[1:,4:]],1)\n",
    "train_size = int(data.shape[0]*0.9)\n",
    "data_train = rate_data[0:train_size]\n",
    "data_test = rate_data[train_size-4:]\n",
    "\n",
    "\n",
    "t0 = 5\n",
    "to_predict = 1\n",
    "train_dataset = GetData(data_train[:,1],torch.cat([data_train[:,0].unsqueeze(1),data_train[:,2:]],1),t0 = t0, to_predict = to_predict)\n",
    "test_dataset = GetData(data_test[:,1],torch.cat([data_test[:,0].unsqueeze(1),data_test[:,2:]],1),t0 = t0, to_predict = to_predict)\n",
    "criterion = torch.nn.MSELoss()\n",
    "# dataloaders\n",
    "train_dl = DataLoader(train_dataset,batch_size=256,shuffle=True)\n",
    "test_dl = DataLoader(test_dataset,batch_size=512)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = TransformerTimeSeries(feature_dim = feature_dim,\n",
    "                              nhead = 4,num_layers = 12,d_model = 512,\n",
    "                              conv_len = 5,feature_weight = True,embedding_weight = True)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5 ,momentum = 0.9, nesterov = True)#Adam optimizer\n",
    "#scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer,5e-7,3e-5,10,mode = 'exp_range',gamma = 0.9996)\n",
    "\n",
    "\n",
    "\n",
    "train_epoch_loss = []\n",
    "test_epoch_loss = []\n",
    "test_best = 9999\n",
    "for epoch in tqdm(range(4000)):\n",
    "\n",
    "\n",
    "    l_t = train_epoch(model,train_dl,optimizer,t0,to_predict)\n",
    "\n",
    "    train_loss = test_epoch(model,train_dl,t0,to_predict)\n",
    "\n",
    "\n",
    "    test_loss = test_epoch(model,test_dl,t0,to_predict)\n",
    "\n",
    "\n",
    "    if test_best > test_loss:\n",
    "        test_best = test_loss\n",
    "        torch.save(model.state_dict(), 'AMZN_H')\n",
    "\n",
    "\n",
    "\n",
    "    train_epoch_loss.append(np.mean(train_loss))\n",
    "    test_epoch_loss.append(test_loss)\n",
    "\n",
    "    print(\"Epoch {}: Train loss: {} \\t test loss: {}\".format(epoch+1,\n",
    "                                                             np.mean(train_loss),\n",
    "                                                             test_loss))\n",
    "    #scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81660eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bmodel = TransformerTimeSeries(feature_dim = feature_dim,\n",
    "                                  nhead = 4,num_layers = 12,d_model = 512,\n",
    "                                  conv_len = 5,feature_weight = True,embedding_weight = True)\n",
    "bmodel.load_state_dict(torch.load('AMZN_H'))\n",
    "bmodel.to(device)\n",
    "bmodel.eval()\n",
    "with torch.no_grad(): # eliminate gradient i.e. gradients only exists in train()\n",
    "\n",
    "    for step,(x,y,feature,attention_masks) in enumerate(test_dl):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        feature = feature.to(device)\n",
    "        \n",
    "        #y = torch.cat([y[:,0:5],torch.ones(47,1).cuda()],1)\n",
    "        #feature = torch.cat([feature[:,:,0:5],torch.ones(47,90,1).cuda()],2)\n",
    "        \n",
    "        attention_masks = attention_masks.to(device)\n",
    "        output = bmodel(x,y,feature,attention_masks[0])\n",
    "        \n",
    "        pred = data[train_size + 1 :-1,1]*0.01*(100+output[:,t0:].squeeze()).detach().cpu()\n",
    "        real = data[train_size + 1 :-1,1]*0.01*(100+y[:,t0:].squeeze()).detach().cpu()\n",
    "\n",
    "        plt.plot(pred)\n",
    "        plt.plot(real)\n",
    "        plt.legend(['pred','real'])\n",
    "        MAE = torch.nn.L1Loss()(pred,real).item()\n",
    "        MSE = torch.nn.MSELoss()(pred,real).item()\n",
    "        RMSE = np.sqrt(MSE)\n",
    "        \n",
    "        print(f'AMZN MAE is {MAE}')\n",
    "        print(f'AMZN MSE is {MSE}')\n",
    "        print(f'AMZN RMSE is {RMSE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7199c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66c80f28",
   "metadata": {},
   "source": [
    "#### low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738a7c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'AMZN_features.csv'\n",
    "with open(file_name) as f:\n",
    "    data_list = csv.reader(f)\n",
    "    data = []\n",
    "    for row in data_list:\n",
    "        data.append(row)\n",
    "    f.close()\n",
    "    \n",
    "data = torch.tensor(np.delete(np.array(data)[1:,2:],4,axis = 1).astype(np.float)).type(torch.float)\n",
    "\n",
    "feature_dim = data.shape[1] - 1\n",
    "rate = []\n",
    "for i in range(1,data.shape[0]):\n",
    "    rate.append((100*(data[i,0:4] - data[i-1,0:4])/data[i-1,0:4]).reshape(1,4))\n",
    "    \n",
    "rate = torch.cat(rate)\n",
    "rate_data = torch.cat([rate,data[1:,4:]],1)\n",
    "train_size = int(data.shape[0]*0.9)\n",
    "data_train = rate_data[0:train_size]\n",
    "data_test = rate_data[train_size-4:]\n",
    "\n",
    "\n",
    "t0 = 5\n",
    "to_predict = 1\n",
    "train_dataset = GetData(data_train[:,2],torch.cat([data_train[:,0:2],data_train[:,3:]],1),t0 = t0, to_predict = to_predict)\n",
    "test_dataset = GetData(data_test[:,2],torch.cat([data_test[:,0:2],data_test[:,3:]],1),t0 = t0, to_predict = to_predict)\n",
    "criterion = torch.nn.MSELoss()\n",
    "# dataloaders\n",
    "train_dl = DataLoader(train_dataset,batch_size=256,shuffle=True)\n",
    "test_dl = DataLoader(test_dataset,batch_size=512)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = TransformerTimeSeries(feature_dim = feature_dim,\n",
    "                              nhead = 4,num_layers = 12,d_model = 512,\n",
    "                              conv_len = 5,feature_weight = True,embedding_weight = True)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5 ,momentum = 0.9, nesterov = True)#Adam optimizer\n",
    "#scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer,5e-7,3e-5,10,mode = 'exp_range',gamma = 0.9996)\n",
    "\n",
    "\n",
    "\n",
    "train_epoch_loss = []\n",
    "test_epoch_loss = []\n",
    "test_best = 9999\n",
    "for epoch in tqdm(range(4000)):\n",
    "\n",
    "\n",
    "    l_t = train_epoch(model,train_dl,optimizer,t0,to_predict)\n",
    "\n",
    "    train_loss = test_epoch(model,train_dl,t0,to_predict)\n",
    "\n",
    "\n",
    "    test_loss = test_epoch(model,test_dl,t0,to_predict)\n",
    "\n",
    "\n",
    "    if test_best > test_loss:\n",
    "        test_best = test_loss\n",
    "        torch.save(model.state_dict(), 'AMZN_L')\n",
    "\n",
    "\n",
    "\n",
    "    train_epoch_loss.append(np.mean(train_loss))\n",
    "    test_epoch_loss.append(test_loss)\n",
    "\n",
    "    print(\"Epoch {}: Train loss: {} \\t test loss: {}\".format(epoch+1,\n",
    "                                                             np.mean(train_loss),\n",
    "                                                             test_loss))\n",
    "    #scheduler.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f359e6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bmodel = TransformerTimeSeries(feature_dim = feature_dim,\n",
    "                                  nhead = 4,num_layers = 12,d_model = 512,\n",
    "                                  conv_len = 5,feature_weight = True,embedding_weight = True)\n",
    "bmodel.load_state_dict(torch.load('AMZN_L'))\n",
    "bmodel.to(device)\n",
    "bmodel.eval()\n",
    "with torch.no_grad(): # eliminate gradient i.e. gradients only exists in train()\n",
    "\n",
    "    for step,(x,y,feature,attention_masks) in enumerate(test_dl):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        feature = feature.to(device)\n",
    "        \n",
    "        #y = torch.cat([y[:,0:5],torch.ones(47,1).cuda()],1)\n",
    "        #feature = torch.cat([feature[:,:,0:5],torch.ones(47,90,1).cuda()],2)\n",
    "        \n",
    "        attention_masks = attention_masks.to(device)\n",
    "        output = bmodel(x,y,feature,attention_masks[0])\n",
    "        \n",
    "        pred = data[train_size + 1 :-1,2]*0.01*(100+output[:,t0:].squeeze()).detach().cpu()\n",
    "        real = data[train_size + 1 :-1,2]*0.01*(100+y[:,t0:].squeeze()).detach().cpu()\n",
    "\n",
    "        plt.plot(pred)\n",
    "        plt.plot(real)\n",
    "        plt.legend(['pred','real'])\n",
    "        MAE = torch.nn.L1Loss()(pred,real).item()\n",
    "        MSE = torch.nn.MSELoss()(pred,real).item()\n",
    "        RMSE = np.sqrt(MSE)\n",
    "        \n",
    "        print(f'AMZN MAE is {MAE}')\n",
    "        print(f'AMZN MSE is {MSE}')\n",
    "        print(f'AMZN RMSE is {RMSE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8331ff19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca48241a",
   "metadata": {},
   "source": [
    "##### close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723733ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'AMZN_features.csv'\n",
    "with open(file_name) as f:\n",
    "    data_list = csv.reader(f)\n",
    "    data = []\n",
    "    for row in data_list:\n",
    "        data.append(row)\n",
    "    f.close()\n",
    "    \n",
    "data = torch.tensor(np.delete(np.array(data)[1:,2:],4,axis = 1).astype(np.float)).type(torch.float)\n",
    "\n",
    "feature_dim = data.shape[1] - 1\n",
    "rate = []\n",
    "for i in range(1,data.shape[0]):\n",
    "    rate.append((100*(data[i,0:4] - data[i-1,0:4])/data[i-1,0:4]).reshape(1,4))\n",
    "    \n",
    "rate = torch.cat(rate)\n",
    "rate_data = torch.cat([rate,data[1:,4:]],1)\n",
    "train_size = int(data.shape[0]*0.9)\n",
    "data_train = rate_data[0:train_size]\n",
    "data_test = rate_data[train_size-4:]\n",
    "\n",
    "\n",
    "t0 = 5\n",
    "to_predict = 1\n",
    "train_dataset = GetData(data_train[:,3],torch.cat([data_train[:,0:3],data_train[:,4:]],1),t0 = t0, to_predict = to_predict)\n",
    "test_dataset = GetData(data_test[:,3],torch.cat([data_test[:,0:3],data_test[:,4:]],1),t0 = t0, to_predict = to_predict)\n",
    "criterion = torch.nn.MSELoss()\n",
    "# dataloaders\n",
    "train_dl = DataLoader(train_dataset,batch_size=256,shuffle=True)\n",
    "test_dl = DataLoader(test_dataset,batch_size=512)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = TransformerTimeSeries(feature_dim = feature_dim,\n",
    "                              nhead = 4,num_layers = 12,d_model = 512,\n",
    "                              conv_len = 5,feature_weight = True,embedding_weight = True)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5 ,momentum = 0.9, nesterov = True)#Adam optimizer\n",
    "#scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer,5e-7,3e-5,10,mode = 'exp_range',gamma = 0.9996)\n",
    "\n",
    "\n",
    "\n",
    "train_epoch_loss = []\n",
    "test_epoch_loss = []\n",
    "test_best = 9999\n",
    "for epoch in tqdm(range(4000)):\n",
    "\n",
    "\n",
    "    l_t = train_epoch(model,train_dl,optimizer,t0,to_predict)\n",
    "\n",
    "    train_loss = test_epoch(model,train_dl,t0,to_predict)\n",
    "\n",
    "\n",
    "    test_loss = test_epoch(model,test_dl,t0,to_predict)\n",
    "\n",
    "\n",
    "    if test_best > test_loss:\n",
    "        test_best = test_loss\n",
    "        torch.save(model.state_dict(), 'AMZN_C')\n",
    "\n",
    "\n",
    "\n",
    "    train_epoch_loss.append(np.mean(train_loss))\n",
    "    test_epoch_loss.append(test_loss)\n",
    "\n",
    "    print(\"Epoch {}: Train loss: {} \\t test loss: {}\".format(epoch+1,\n",
    "                                                             np.mean(train_loss),\n",
    "                                                             test_loss))\n",
    "    #scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c29749",
   "metadata": {},
   "outputs": [],
   "source": [
    "bmodel = TransformerTimeSeries(feature_dim = feature_dim,\n",
    "                                  nhead = 4,num_layers = 12,d_model = 512,\n",
    "                                  conv_len = 5,feature_weight = True,embedding_weight = True)\n",
    "bmodel.load_state_dict(torch.load('AMZN_C'))\n",
    "bmodel.to(device)\n",
    "bmodel.eval()\n",
    "with torch.no_grad(): # eliminate gradient i.e. gradients only exists in train()\n",
    "\n",
    "    for step,(x,y,feature,attention_masks) in enumerate(test_dl):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        feature = feature.to(device)\n",
    "        \n",
    "        #y = torch.cat([y[:,0:5],torch.ones(47,1).cuda()],1)\n",
    "        #feature = torch.cat([feature[:,:,0:5],torch.ones(47,90,1).cuda()],2)\n",
    "        \n",
    "        attention_masks = attention_masks.to(device)\n",
    "        output = bmodel(x,y,feature,attention_masks[0])\n",
    "        \n",
    "        pred = data[train_size + 1 :-1,3]*0.01*(100+output[:,t0:].squeeze()).detach().cpu()\n",
    "        real = data[train_size + 1 :-1,3]*0.01*(100+y[:,t0:].squeeze()).detach().cpu()\n",
    "\n",
    "        plt.plot(pred)\n",
    "        plt.plot(real)\n",
    "        plt.legend(['pred','real'])\n",
    "        MAE = torch.nn.L1Loss()(pred,real).item()\n",
    "        MSE = torch.nn.MSELoss()(pred,real).item()\n",
    "        RMSE = np.sqrt(MSE)\n",
    "        \n",
    "        print(f'AMZN MAE is {MAE}')\n",
    "        print(f'AMZN MSE is {MSE}')\n",
    "        print(f'AMZN RMSE is {RMSE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f29f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401f3864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ef74653",
   "metadata": {},
   "source": [
    "## GOOG\n",
    "##### open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdda670",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'GOOG_features.csv'\n",
    "with open(file_name) as f:\n",
    "    data_list = csv.reader(f)\n",
    "    data = []\n",
    "    for row in data_list:\n",
    "        data.append(row)\n",
    "    f.close()\n",
    "    \n",
    "data = torch.tensor(np.delete(np.array(data)[1:,2:],4,axis = 1).astype(np.float)).type(torch.float)\n",
    "\n",
    "feature_dim = data.shape[1] - 1\n",
    "rate = []\n",
    "for i in range(1,data.shape[0]):\n",
    "    rate.append((100*(data[i,0:4] - data[i-1,0:4])/data[i-1,0:4]).reshape(1,4))\n",
    "    \n",
    "rate = torch.cat(rate)\n",
    "rate_data = torch.cat([rate,data[1:,4:]],1)\n",
    "train_size = int(data.shape[0]*0.9)\n",
    "data_train = rate_data[0:train_size]\n",
    "data_test = rate_data[train_size-4:]\n",
    "\n",
    "\n",
    "t0 = 5\n",
    "to_predict = 1\n",
    "train_dataset = GetData(data_train[:,0],data_train[:,1:],t0 = t0, to_predict = to_predict)\n",
    "test_dataset = GetData(data_test[:,0],data_test[:,1:],t0 = t0, to_predict = to_predict)\n",
    "criterion = torch.nn.MSELoss()\n",
    "# dataloaders\n",
    "train_dl = DataLoader(train_dataset,batch_size=256,shuffle=True)\n",
    "test_dl = DataLoader(test_dataset,batch_size=512)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = TransformerTimeSeries(feature_dim = feature_dim,\n",
    "                              nhead = 4,num_layers = 12,d_model = 512,\n",
    "                              conv_len = 5,feature_weight = True,embedding_weight = True)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5 ,momentum = 0.9, nesterov = True)#Adam optimizer\n",
    "#scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer,5e-7,3e-5,10,mode = 'exp_range',gamma = 0.9996)\n",
    "\n",
    "\n",
    "\n",
    "train_epoch_loss = []\n",
    "test_epoch_loss = []\n",
    "test_best = 9999\n",
    "for epoch in tqdm(range(4000)):\n",
    "\n",
    "\n",
    "    l_t = train_epoch(model,train_dl,optimizer,t0,to_predict)\n",
    "\n",
    "    train_loss = test_epoch(model,train_dl,t0,to_predict)\n",
    "\n",
    "\n",
    "    test_loss = test_epoch(model,test_dl,t0,to_predict)\n",
    "\n",
    "\n",
    "    if test_best > test_loss:\n",
    "        test_best = test_loss\n",
    "        torch.save(model.state_dict(), 'GOOG')\n",
    "\n",
    "\n",
    "\n",
    "    train_epoch_loss.append(np.mean(train_loss))\n",
    "    test_epoch_loss.append(test_loss)\n",
    "\n",
    "    print(\"Epoch {}: Train loss: {} \\t test loss: {}\".format(epoch+1,\n",
    "                                                             np.mean(train_loss),\n",
    "                                                             test_loss))\n",
    "    #scheduler.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f44763",
   "metadata": {},
   "outputs": [],
   "source": [
    "bmodel = TransformerTimeSeries(feature_dim = feature_dim,\n",
    "                                  nhead = 4,num_layers = 12,d_model = 512,\n",
    "                                  conv_len = 5,feature_weight = True,embedding_weight = True)\n",
    "bmodel.load_state_dict(torch.load('GOOG'))\n",
    "bmodel.to(device)\n",
    "bmodel.eval()\n",
    "with torch.no_grad(): # eliminate gradient i.e. gradients only exists in train()\n",
    "\n",
    "    for step,(x,y,feature,attention_masks) in enumerate(test_dl):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        feature = feature.to(device)\n",
    "        attention_masks = attention_masks.to(device)\n",
    "        output = bmodel(x,y,feature,attention_masks[0])\n",
    "        \n",
    "        pred = data[train_size + 1 :-1,0]*0.01*(100+output[:,t0:].squeeze()).detach().cpu()\n",
    "        real = data[train_size + 1 :-1,0]*0.01*(100+y[:,t0:].squeeze()).detach().cpu()\n",
    "\n",
    "        plt.plot(pred)\n",
    "        plt.plot(real)\n",
    "        plt.legend(['pred','real'])\n",
    "        MAE = torch.nn.L1Loss()(pred,real).item()\n",
    "        MSE = torch.nn.MSELoss()(pred,real).item()\n",
    "        RMSE = np.sqrt(MSE)\n",
    "        \n",
    "        print(f'GOOG MAE is {MAE}')\n",
    "        print(f'GOOG MSE is {MSE}')\n",
    "        print(f'GOOG RMSE is {RMSE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c95195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdd30674",
   "metadata": {},
   "source": [
    "##### High"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5947ce2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'GOOG_features.csv'\n",
    "with open(file_name) as f:\n",
    "    data_list = csv.reader(f)\n",
    "    data = []\n",
    "    for row in data_list:\n",
    "        data.append(row)\n",
    "    f.close()\n",
    "    \n",
    "data = torch.tensor(np.delete(np.array(data)[1:,2:],4,axis = 1).astype(np.float)).type(torch.float)\n",
    "\n",
    "feature_dim = data.shape[1] - 1\n",
    "rate = []\n",
    "for i in range(1,data.shape[0]):\n",
    "    rate.append((100*(data[i,0:4] - data[i-1,0:4])/data[i-1,0:4]).reshape(1,4))\n",
    "    \n",
    "rate = torch.cat(rate)\n",
    "rate_data = torch.cat([rate,data[1:,4:]],1)\n",
    "train_size = int(data.shape[0]*0.9)\n",
    "data_train = rate_data[0:train_size]\n",
    "data_test = rate_data[train_size-4:]\n",
    "\n",
    "\n",
    "t0 = 5\n",
    "to_predict = 1\n",
    "train_dataset = GetData(data_train[:,1],torch.cat([data_train[:,0].unsqueeze(1),data_train[:,2:]],1),t0 = t0, to_predict = to_predict)\n",
    "test_dataset = GetData(data_test[:,1],torch.cat([data_test[:,0].unsqueeze(1),data_test[:,2:]],1),t0 = t0, to_predict = to_predict)\n",
    "criterion = torch.nn.MSELoss()\n",
    "# dataloaders\n",
    "train_dl = DataLoader(train_dataset,batch_size=256,shuffle=True)\n",
    "test_dl = DataLoader(test_dataset,batch_size=512)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = TransformerTimeSeries(feature_dim = feature_dim,\n",
    "                              nhead = 4,num_layers = 12,d_model = 512,\n",
    "                              conv_len = 5,feature_weight = True,embedding_weight = True)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5 ,momentum = 0.9, nesterov = True)#Adam optimizer\n",
    "#scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer,5e-7,3e-5,10,mode = 'exp_range',gamma = 0.9996)\n",
    "\n",
    "\n",
    "\n",
    "train_epoch_loss = []\n",
    "test_epoch_loss = []\n",
    "test_best = 9999\n",
    "for epoch in tqdm(range(4000)):\n",
    "\n",
    "\n",
    "    l_t = train_epoch(model,train_dl,optimizer,t0,to_predict)\n",
    "\n",
    "    train_loss = test_epoch(model,train_dl,t0,to_predict)\n",
    "\n",
    "\n",
    "    test_loss = test_epoch(model,test_dl,t0,to_predict)\n",
    "\n",
    "\n",
    "    if test_best > test_loss:\n",
    "        test_best = test_loss\n",
    "        torch.save(model.state_dict(), 'GOOG_H')\n",
    "\n",
    "\n",
    "\n",
    "    train_epoch_loss.append(np.mean(train_loss))\n",
    "    test_epoch_loss.append(test_loss)\n",
    "\n",
    "    print(\"Epoch {}: Train loss: {} \\t test loss: {}\".format(epoch+1,\n",
    "                                                             np.mean(train_loss),\n",
    "                                                             test_loss))\n",
    "    #scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76979d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bmodel = TransformerTimeSeries(feature_dim = feature_dim,\n",
    "                                  nhead = 4,num_layers = 12,d_model = 512,\n",
    "                                  conv_len = 5,feature_weight = True,embedding_weight = True)\n",
    "bmodel.load_state_dict(torch.load('GOOG_H'))\n",
    "bmodel.to(device)\n",
    "bmodel.eval()\n",
    "with torch.no_grad(): # eliminate gradient i.e. gradients only exists in train()\n",
    "\n",
    "    for step,(x,y,feature,attention_masks) in enumerate(test_dl):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        feature = feature.to(device)\n",
    "        attention_masks = attention_masks.to(device)\n",
    "        output = bmodel(x,y,feature,attention_masks[0])\n",
    "        \n",
    "        pred = data[train_size + 1 :-1,1]*0.01*(100+output[:,t0:].squeeze()).detach().cpu()\n",
    "        real = data[train_size + 1 :-1,1]*0.01*(100+y[:,t0:].squeeze()).detach().cpu()\n",
    "\n",
    "        plt.plot(pred)\n",
    "        plt.plot(real)\n",
    "        plt.legend(['pred','real'])\n",
    "        MAE = torch.nn.L1Loss()(pred,real).item()\n",
    "        MSE = torch.nn.MSELoss()(pred,real).item()\n",
    "        RMSE = np.sqrt(MSE)\n",
    "        \n",
    "        print(f'GOOG MAE is {MAE}')\n",
    "        print(f'GOOG MSE is {MSE}')\n",
    "        print(f'GOOG RMSE is {RMSE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca38c317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2406e5af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cec931d",
   "metadata": {},
   "source": [
    "##### Low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d587879",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'GOOG_features.csv'\n",
    "with open(file_name) as f:\n",
    "    data_list = csv.reader(f)\n",
    "    data = []\n",
    "    for row in data_list:\n",
    "        data.append(row)\n",
    "    f.close()\n",
    "    \n",
    "data = torch.tensor(np.delete(np.array(data)[1:,2:],4,axis = 1).astype(np.float)).type(torch.float)\n",
    "\n",
    "feature_dim = data.shape[1] - 1\n",
    "rate = []\n",
    "for i in range(1,data.shape[0]):\n",
    "    rate.append((100*(data[i,0:4] - data[i-1,0:4])/data[i-1,0:4]).reshape(1,4))\n",
    "    \n",
    "rate = torch.cat(rate)\n",
    "rate_data = torch.cat([rate,data[1:,4:]],1)\n",
    "train_size = int(data.shape[0]*0.9)\n",
    "data_train = rate_data[0:train_size]\n",
    "data_test = rate_data[train_size-4:]\n",
    "\n",
    "\n",
    "t0 = 5\n",
    "to_predict = 1\n",
    "train_dataset = GetData(data_train[:,2],torch.cat([data_train[:,0:2],data_train[:,3:]],1),t0 = t0, to_predict = to_predict)\n",
    "test_dataset = GetData(data_test[:,2],torch.cat([data_test[:,0:2],data_test[:,3:]],1),t0 = t0, to_predict = to_predict)\n",
    "criterion = torch.nn.MSELoss()\n",
    "# dataloaders\n",
    "train_dl = DataLoader(train_dataset,batch_size=256,shuffle=True)\n",
    "test_dl = DataLoader(test_dataset,batch_size=512)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = TransformerTimeSeries(feature_dim = feature_dim,\n",
    "                              nhead = 4,num_layers = 12,d_model = 512,\n",
    "                              conv_len = 5,feature_weight = True,embedding_weight = True)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5 ,momentum = 0.9, nesterov = True)#Adam optimizer\n",
    "#scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer,5e-7,3e-5,10,mode = 'exp_range',gamma = 0.9996)\n",
    "\n",
    "\n",
    "\n",
    "train_epoch_loss = []\n",
    "test_epoch_loss = []\n",
    "test_best = 9999\n",
    "for epoch in tqdm(range(4000)):\n",
    "\n",
    "\n",
    "    l_t = train_epoch(model,train_dl,optimizer,t0,to_predict)\n",
    "\n",
    "    train_loss = test_epoch(model,train_dl,t0,to_predict)\n",
    "\n",
    "\n",
    "    test_loss = test_epoch(model,test_dl,t0,to_predict)\n",
    "\n",
    "\n",
    "    if test_best > test_loss:\n",
    "        test_best = test_loss\n",
    "        torch.save(model.state_dict(), 'GOOG_L')\n",
    "\n",
    "\n",
    "\n",
    "    train_epoch_loss.append(np.mean(train_loss))\n",
    "    test_epoch_loss.append(test_loss)\n",
    "\n",
    "    print(\"Epoch {}: Train loss: {} \\t test loss: {}\".format(epoch+1,\n",
    "                                                             np.mean(train_loss),\n",
    "                                                             test_loss))\n",
    "    #scheduler.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5efa51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bmodel = TransformerTimeSeries(feature_dim = feature_dim,\n",
    "                                  nhead = 4,num_layers = 12,d_model = 512,\n",
    "                                  conv_len = 5,feature_weight = True,embedding_weight = True)\n",
    "bmodel.load_state_dict(torch.load('GOOG_L'))\n",
    "bmodel.to(device)\n",
    "bmodel.eval()\n",
    "with torch.no_grad(): # eliminate gradient i.e. gradients only exists in train()\n",
    "\n",
    "    for step,(x,y,feature,attention_masks) in enumerate(test_dl):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        feature = feature.to(device)\n",
    "        attention_masks = attention_masks.to(device)\n",
    "        output = bmodel(x,y,feature,attention_masks[0])\n",
    "        \n",
    "        pred = data[train_size + 1 :-1,2]*0.01*(100+output[:,t0:].squeeze()).detach().cpu()\n",
    "        real = data[train_size + 1 :-1,2]*0.01*(100+y[:,t0:].squeeze()).detach().cpu()\n",
    "\n",
    "        plt.plot(pred)\n",
    "        plt.plot(real)\n",
    "        plt.legend(['pred','real'])\n",
    "        MAE = torch.nn.L1Loss()(pred,real).item()\n",
    "        MSE = torch.nn.MSELoss()(pred,real).item()\n",
    "        RMSE = np.sqrt(MSE)\n",
    "        \n",
    "        print(f'GOOG MAE is {MAE}')\n",
    "        print(f'GOOG MSE is {MSE}')\n",
    "        print(f'GOOG RMSE is {RMSE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650a3a93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b67d50e",
   "metadata": {},
   "source": [
    "##### Close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fa6bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'GOOG_features.csv'\n",
    "with open(file_name) as f:\n",
    "    data_list = csv.reader(f)\n",
    "    data = []\n",
    "    for row in data_list:\n",
    "        data.append(row)\n",
    "    f.close()\n",
    "    \n",
    "data = torch.tensor(np.delete(np.array(data)[1:,2:],4,axis = 1).astype(np.float)).type(torch.float)\n",
    "\n",
    "feature_dim = data.shape[1] - 1\n",
    "rate = []\n",
    "for i in range(1,data.shape[0]):\n",
    "    rate.append((100*(data[i,0:4] - data[i-1,0:4])/data[i-1,0:4]).reshape(1,4))\n",
    "    \n",
    "rate = torch.cat(rate)\n",
    "rate_data = torch.cat([rate,data[1:,4:]],1)\n",
    "train_size = int(data.shape[0]*0.9)\n",
    "data_train = rate_data[0:train_size]\n",
    "data_test = rate_data[train_size-4:]\n",
    "\n",
    "\n",
    "t0 = 5\n",
    "to_predict = 1\n",
    "train_dataset = GetData(data_train[:,3],torch.cat([data_train[:,0:3],data_train[:,4:]],1),t0 = t0, to_predict = to_predict)\n",
    "test_dataset = GetData(data_test[:,3],torch.cat([data_test[:,0:3],data_test[:,4:]],1),t0 = t0, to_predict = to_predict)\n",
    "criterion = torch.nn.MSELoss()\n",
    "# dataloaders\n",
    "train_dl = DataLoader(train_dataset,batch_size=256,shuffle=True)\n",
    "test_dl = DataLoader(test_dataset,batch_size=512)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = TransformerTimeSeries(feature_dim = feature_dim,\n",
    "                              nhead = 4,num_layers = 12,d_model = 512,\n",
    "                              conv_len = 5,feature_weight = True,embedding_weight = True)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5 ,momentum = 0.9, nesterov = True)#Adam optimizer\n",
    "#scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer,5e-7,3e-5,10,mode = 'exp_range',gamma = 0.9996)\n",
    "\n",
    "\n",
    "\n",
    "train_epoch_loss = []\n",
    "test_epoch_loss = []\n",
    "test_best = 9999\n",
    "for epoch in tqdm(range(4000)):\n",
    "\n",
    "\n",
    "    l_t = train_epoch(model,train_dl,optimizer,t0,to_predict)\n",
    "\n",
    "    train_loss = test_epoch(model,train_dl,t0,to_predict)\n",
    "\n",
    "\n",
    "    test_loss = test_epoch(model,test_dl,t0,to_predict)\n",
    "\n",
    "\n",
    "    if test_best > test_loss:\n",
    "        test_best = test_loss\n",
    "        torch.save(model.state_dict(), 'GOOG_C')\n",
    "\n",
    "\n",
    "\n",
    "    train_epoch_loss.append(np.mean(train_loss))\n",
    "    test_epoch_loss.append(test_loss)\n",
    "\n",
    "    print(\"Epoch {}: Train loss: {} \\t test loss: {}\".format(epoch+1,\n",
    "                                                             np.mean(train_loss),\n",
    "                                                             test_loss))\n",
    "    #scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d47af62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2786e3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bmodel = TransformerTimeSeries(feature_dim = feature_dim,\n",
    "                                  nhead = 4,num_layers = 12,d_model = 512,\n",
    "                                  conv_len = 5,feature_weight = True,embedding_weight = True)\n",
    "bmodel.load_state_dict(torch.load('GOOG_C'))\n",
    "bmodel.to(device)\n",
    "bmodel.eval()\n",
    "with torch.no_grad(): # eliminate gradient i.e. gradients only exists in train()\n",
    "\n",
    "    for step,(x,y,feature,attention_masks) in enumerate(test_dl):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        feature = feature.to(device)\n",
    "        attention_masks = attention_masks.to(device)\n",
    "        output = bmodel(x,y,feature,attention_masks[0])\n",
    "        \n",
    "        \n",
    "        pred = data[train_size + 1 :-1,3]*0.01*(100+output[:,t0:].squeeze()).detach().cpu()\n",
    "        real = data[train_size + 1 :-1,3]*0.01*(100+y[:,t0:].squeeze()).detach().cpu()\n",
    "\n",
    "        plt.plot(pred)\n",
    "        plt.plot(real)\n",
    "        plt.legend(['pred','real'])\n",
    "        MAE = torch.nn.L1Loss()(pred,real).item()\n",
    "        MSE = torch.nn.MSELoss()(pred,real).item()\n",
    "        RMSE = np.sqrt(MSE)\n",
    "        \n",
    "        print(f'GOOG MAE is {MAE}')\n",
    "        print(f'GOOG MSE is {MSE}')\n",
    "        print(f'GOOG RMSE is {RMSE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4392d433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898af722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0bb98e7",
   "metadata": {},
   "source": [
    "#### 12 year AAPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3d6ee5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_name = 'AAPL_12_features.csv'\n",
    "with open(file_name) as f:\n",
    "    data_list = csv.reader(f)\n",
    "    data = []\n",
    "    for row in data_list:\n",
    "        data.append(row)\n",
    "    f.close()\n",
    "    \n",
    "data = torch.tensor(np.delete(np.array(data)[1:,2:],4,axis = 1).astype(np.float)).type(torch.float)\n",
    "\n",
    "feature_dim = data.shape[1] - 1\n",
    "rate = []\n",
    "for i in range(1,data.shape[0]):\n",
    "    rate.append((100*(data[i,0:4] - data[i-1,0:4])/data[i-1,0:4]).reshape(1,4))\n",
    "    \n",
    "rate = torch.cat(rate)\n",
    "rate_data = torch.cat([rate,data[1:,4:]],1)\n",
    "train_size = int(data.shape[0]*0.9)\n",
    "data_train = rate_data[0:train_size]\n",
    "data_test = rate_data[train_size-4:]\n",
    "\n",
    "\n",
    "t0 = 5\n",
    "to_predict = 1\n",
    "train_dataset = GetData(data_train[:,3],torch.cat([data_train[:,0:3],data_train[:,4:]],1),t0 = t0, to_predict = to_predict)\n",
    "test_dataset = GetData(data_test[:,3],torch.cat([data_test[:,0:3],data_test[:,4:]],1),t0 = t0, to_predict = to_predict)\n",
    "criterion = torch.nn.MSELoss()\n",
    "# dataloaders\n",
    "train_dl = DataLoader(train_dataset,batch_size=256,shuffle=True)\n",
    "test_dl = DataLoader(test_dataset,batch_size=512)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = TransformerTimeSeries(feature_dim = feature_dim,\n",
    "                              nhead = 4,num_layers = 12,d_model = 512,\n",
    "                              conv_len = 5,feature_weight = True,embedding_weight = True)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5 ,momentum = 0.9, nesterov = True)#Adam optimizer\n",
    "#scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer,5e-7,3e-5,10,mode = 'exp_range',gamma = 0.9996)\n",
    "\n",
    "\n",
    "\n",
    "train_epoch_loss = []\n",
    "test_epoch_loss = []\n",
    "test_best = 9999\n",
    "for epoch in tqdm(range(4000)):\n",
    "\n",
    "\n",
    "    l_t = train_epoch(model,train_dl,optimizer,t0,to_predict)\n",
    "\n",
    "    train_loss = test_epoch(model,train_dl,t0,to_predict)\n",
    "\n",
    "\n",
    "    test_loss = test_epoch(model,test_dl,t0,to_predict)\n",
    "\n",
    "\n",
    "    if test_best > test_loss:\n",
    "        test_best = test_loss\n",
    "        torch.save(model.state_dict(), 'AAPL_12')\n",
    "\n",
    "\n",
    "\n",
    "    train_epoch_loss.append(np.mean(train_loss))\n",
    "    test_epoch_loss.append(test_loss)\n",
    "\n",
    "    print(\"Epoch {}: Train loss: {} \\t test loss: {}\".format(epoch+1,\n",
    "                                                             np.mean(train_loss),\n",
    "                                                             test_loss))\n",
    "    #scheduler.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c75a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "bmodel = TransformerTimeSeries(feature_dim = feature_dim,\n",
    "                                  nhead = 4,num_layers = 12,d_model = 512,\n",
    "                                  conv_len = 5,feature_weight = True,embedding_weight = True)\n",
    "bmodel.load_state_dict(torch.load('AAPL_12'))\n",
    "bmodel.to(device)\n",
    "bmodel.eval()\n",
    "with torch.no_grad(): # eliminate gradient i.e. gradients only exists in train()\n",
    "\n",
    "    for step,(x,y,feature,attention_masks) in enumerate(test_dl):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        feature = feature.to(device)\n",
    "        attention_masks = attention_masks.to(device)\n",
    "        output = bmodel(x,y,feature,attention_masks[0])\n",
    "        \n",
    "        pred = data[train_size + 1 :-1,3]*0.01*(100+output[:,t0:].squeeze()).detach().cpu()\n",
    "        ## note that test set starts at index train_size + 1 because idex train_size in rate_data \n",
    "        ## is the last day of first test data point, but rate_data only have the second til last data points\n",
    "        ## of the dataset.\n",
    "        #real = data[train_size + 1 :-1,3]*0.01*(100+y[:,t0:].squeeze()).detach().cpu()\n",
    "        real = data[train_size+2:,3].detach().cpu()\n",
    "        plt.plot(pred[-50:])\n",
    "        plt.plot(real[-50:])\n",
    "        plt.legend(['pred','real'])\n",
    "        MAE = torch.nn.L1Loss()(pred,real).item()\n",
    "        MSE = torch.nn.MSELoss()(pred,real).item()\n",
    "        RMSE = np.sqrt(MSE)\n",
    "        \n",
    "        print(f'AAPL MAE is {MAE}')\n",
    "        print(f'AAPL MSE is {MSE}')\n",
    "        print(f'AAPL RMSE is {RMSE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54c3b35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0141d917",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
